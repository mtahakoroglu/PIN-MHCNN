{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tftgglK7JTo-",
        "outputId": "f1630419-8310-4cda-f8f9-d68d3ad99fbe"
      },
      "outputs": [],
      "source": [
        "# multivariate multi-headed 1d cnn example\n",
        "from numpy import array\n",
        "from numpy import hstack\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "from keras.layers import concatenate\n",
        "import math\n",
        "from numpy import array\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import hstack\n",
        "import numpy as np\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import GRU\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import AveragePooling1D \n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
        "import keras.layers\n",
        "import pandas as pd\n",
        "\n",
        "#dataları 1d cnn yapısı için uygun boyuta getirir\n",
        "# split a multivariate sequence into samples\n",
        "def split_sequences(sequences, n_steps):\n",
        "\tX, y1,y2 = list(), list(),list()\n",
        "\tfor i in range(len(sequences)):\n",
        "\t\t# find the end of this pattern\n",
        "\t\tend_ix = i + n_steps\n",
        "\t\t# check if we are beyond the dataset\n",
        "\t\tif end_ix > len(sequences):\n",
        "\t\t\tbreak\n",
        "\t\t# gather input and output parts of the pattern\n",
        "\t\tseq_x, seq_y1,seq_y2 = sequences[i:end_ix, :-2], sequences[end_ix-1, -2],sequences[end_ix-1, -1]\n",
        "\t\tX.append(seq_x)\n",
        "\t\ty1.append(seq_y1)\n",
        "\t\ty2.append(seq_y2)\n",
        "\treturn array(X), array(y1),array(y2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "#dosyadaki imu datalarını oku(train dataset)\n",
        "s1=pd.read_csv('data_inert1.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s2=pd.read_csv('data_inert2.txt', delimiter= '\\s+', index_col=False, header=None)\n",
        "s3=pd.read_csv('data_inert3.txt' , delimiter= '\\s+', index_col=False, header=None)\n",
        "s4=pd.read_csv('data_inert4.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s5=pd.read_csv('data_inert5.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s6=pd.read_csv('data_inert6.txt', delimiter= '\\s+', index_col=False, header=None)\n",
        "s7=pd.read_csv('data_inert7.txt' , delimiter= '\\s+', index_col=False, header=None) #test dataset\n",
        "s8=pd.read_csv('data_inert8.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s9=pd.read_csv('data_inert9.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s10=pd.read_csv('data_inert10.txt', delimiter= '\\s+', index_col=False, header=None)\n",
        "s11=pd.read_csv('data_inert11.txt' , delimiter= '\\s+', index_col=False, header=None)\n",
        "s12=pd.read_csv('data_inert12.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "s13=pd.read_csv('data_inert13.txt',  delimiter= '\\s+', index_col=False, header=None)\n",
        "\n",
        "#dataları birlerştir(train dataset)\n",
        "dat=pd.concat([s1,s2,s3,s4,s5,s6,s8,s9,s10,s11,s12,s13], ignore_index=True)\n",
        "dattest=pd.concat([s7], ignore_index=True)#test dataset\n",
        "\n",
        "#datasette gerekli kısmı seçiyoruz.  1,2,3 numaralı kolonlar ax,ay,az.  4,5,6 numaralı kolonlar gx,gy,gz(tarin dataset son hali)\n",
        "data1=dat[[1]]\n",
        "data2=dat[[2]]\n",
        "data3=dat[[3]]\n",
        "data4=dat[[4]]\n",
        "data5=dat[[5]]\n",
        "data6=dat[[6]]\n",
        "#test datasette gerekli kısmı seçiyoruz.  1,2,3 numaralı kolonlar ax,ay,az.  4,5,6 numaralı kolonlar gx,gy,gz(test dataset son hali)\n",
        "data1test=dattest[[1]]\n",
        "data2test=dattest[[2]]\n",
        "data3test=dattest[[3]]\n",
        "data4test=dattest[[4]]\n",
        "data5test=dattest[[5]]\n",
        "data6test=dattest[[6]]\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train ve test dataları numpy vektöre a çeviriyoruz\n",
        "in_seq1=data1.to_numpy()\n",
        "in_seq2=data2.to_numpy()\n",
        "in_seq3=data3.to_numpy()\n",
        "in_seq4=data4.to_numpy()\n",
        "in_seq5=data5.to_numpy()\n",
        "in_seq6=data6.to_numpy()\n",
        "\n",
        "in_seq1t=data1test.to_numpy()\n",
        "in_seq2t=data2test.to_numpy()\n",
        "in_seq3t=data3test.to_numpy()\n",
        "in_seq4t=data4test.to_numpy()\n",
        "in_seq5t=data5test.to_numpy()\n",
        "in_seq6t=data6test.to_numpy()\n",
        "\n",
        "#target data oku\n",
        "q1=pd.read_csv('resp1.csv')\n",
        "q2=pd.read_csv('resp2.csv')\n",
        "q3=pd.read_csv('resp3.csv')\n",
        "q4=pd.read_csv('resp4.csv')\n",
        "q5=pd.read_csv('resp5.csv')\n",
        "q6=pd.read_csv('resp6.csv')\n",
        "q7=pd.read_csv('resp7.csv')\n",
        "q8=pd.read_csv('resp8.csv')\n",
        "q9=pd.read_csv('resp9.csv')\n",
        "q10=pd.read_csv('resp10.csv')\n",
        "q11=pd.read_csv('resp11.csv')\n",
        "q12=pd.read_csv('resp12.csv')\n",
        "q13=pd.read_csv('resp13.csv')\n",
        "yya=pd.concat([q1,q2,q3,q4,q5,q6,q8,q9,q10,q11,q12,q13], ignore_index=True) # train dataları birleştir\n",
        "yyatest=pd.concat([q7], ignore_index=True)# test dataları birleştir\n",
        "yyatest=yyatest[['1','2','9']]#test target gerekli kısmı seçiyoruz.  1,2,9 numaralı kolonlar pos x, pos y,  yaw (test target son hali)\n",
        "\n",
        "yya=yya[['1','2','9']]#train target gerekli kısmı seçiyoruz.  1,2,9 numaralı kolonlar pos x, pos y,  yaw (train target son hali)\n",
        "\n",
        "#target ları numpy vektöre çevir\n",
        "yyx=yya.to_numpy()\n",
        "yyxtest=yyatest.to_numpy()\n",
        "\n",
        "#aşağıdaki kod dataları filtreler. bunu kullanmadık sonucu belki daha iyi hale getirir \n",
        "'''\n",
        "from scipy.signal import lfilter\n",
        "#filter-------------------------------------------------------------------------------------\n",
        "n = 16  # the larger n is, the smoother curve will be\n",
        "b = [1.0 / n] * n\n",
        "a = 1\n",
        "\n",
        "\n",
        "in_seq1= lfilter(b, a, in_seq1)\n",
        "in_seq2=lfilter(b, a, in_seq2)\n",
        "in_seq3=lfilter(b, a, in_seq3)\n",
        "in_seq4=lfilter(b, a, in_seq4)\n",
        "in_seq5=lfilter(b, a, in_seq5)\n",
        "in_seq6=lfilter(b, a, in_seq6)\n",
        "\n",
        "in_seq1t=lfilter(b, a, in_seq1t)\n",
        "in_seq2t=lfilter(b, a, in_seq2t)\n",
        "in_seq3t=lfilter(b, a, in_seq3t)\n",
        "in_seq4t=lfilter(b, a, in_seq4t)\n",
        "in_seq5t=lfilter(b, a, in_seq5t)\n",
        "in_seq6t=lfilter(b, a, in_seq6t)\n",
        "\n",
        "\n",
        "yyxtest[:,1] = lfilter(b, a, yyxtest[:,1])\n",
        "yyxtest[:,0] = lfilter(b, a, yyxtest[:,0])\n",
        "yyxtest[:,2] = lfilter(b, a, yyxtest[:,2])\n",
        "yyx[:,1] = lfilter(b, a, yyx[:,1])\n",
        "yyx[:,0] = lfilter(b, a, yyx[:,0])\n",
        "yyx[:,2] = lfilter(b, a, yyx[:,2])\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "\n",
        "#train datada deltaposition normunu hesaplar. bunu target olarak kullanacağız\n",
        "f=yyx.shape\n",
        "a=np.zeros((f[0], 1))\n",
        "ttt=0\n",
        "for i in range(f[0]-1):\n",
        "    a[i,0]=math.sqrt((yyx[i+1,0]-yyx[i,0])**2+(yyx[i+1,1]-yyx[i,1])**2)\n",
        "#----------------------------\n",
        "#f=yyx.shape\n",
        "#dmag=np.zeros((f[0], 1))\n",
        "#ttt=0\n",
        "#for i in range(f[0]-1):\n",
        "#    dmag[i,0]=a[i+1,0]-a[i,0]\n",
        "\n",
        "#test datada deltaposition normunu hesaplar. bunu normu tahmin etmek çin target olarak kullanacağız\n",
        "\n",
        "f=yyxtest.shape\n",
        "at=np.zeros((f[0], 1))\n",
        "for i in range(f[0]-1):\n",
        "    at[i,0]=math.sqrt((yyxtest[i+1,0]-yyxtest[i,0])**2+(yyxtest[i+1,1]-yyxtest[i,1])**2)\n",
        "#---------------------\n",
        "#f=yyxtest.shape\n",
        "#dmagt=np.zeros((f[0], 1))\n",
        "#ttt=0\n",
        "#for i in range(f[0]-1):\n",
        " #   dmagt[i,0]=at[i+1,0]-at[i,0]\n",
        "\n",
        "\n",
        "#test datada deltayaw hesaplar. bunu heading i tahmin etmek için target olarak kullanacağız\n",
        "\n",
        "arr = np.arange(0,len(yyxtest))\n",
        "f=yyxtest.shape\n",
        "rrt=np.zeros((f[0], 1))\n",
        "for i in range(f[0]-1):\n",
        "    rrt[i,0]=yyxtest[i+1,2]-yyxtest[i,2]\n",
        "\n",
        "\n",
        "#train datada deltayaw hesaplar. bunu heading i tahmin etmek için target olarak kullanacağız\n",
        "yyp=yyx.shape\n",
        "rr=np.zeros((yyp[0], 1))\n",
        "for i in range(yyp[0]-1):\n",
        "    rr[i,0]=yyx[i+1,2]-yyx[i,2]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#rr->yaw of train,rrt->yaw of test,at->mag of test,a->mag of train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#bu kısım oluşturulan target data(pos.norm,heading) matlabdaki çıktıyı veriyormu diye baktık. biraz farklı sonuç verdi. \n",
        "arr = np.arange(0,len(yyxtest[:,2]))\n",
        "sx=np.zeros(len(yyxtest[:,2]))\n",
        "sy=np.zeros(len(yyxtest[:,2]))\n",
        "\n",
        "for i in arr:\n",
        "    sx[i]=at[i]*math.cos(yyxtest[i,2])\n",
        "    sy[i]=at[i]*math.sin(yyxtest[i,2])\n",
        "\n",
        "ddd=0\n",
        "vvv=0\n",
        "arr = np.arange(0,len(sx))\n",
        "xft=np.zeros((len(sx),2))\n",
        "\n",
        "\n",
        "for i in arr:\n",
        "    xft[i,0]=ddd+sx[i]\n",
        "    xft[i,1]=vvv+sy[i]\n",
        "    \n",
        "    ddd=xft[i,0]\n",
        "    vvv=xft[i,1]\n",
        "\n",
        "plt.scatter(xft[:,0],xft[:,1],linewidth=2, linestyle=\"-\", c=\"r\")\n",
        "\n",
        "plt.scatter(yyxtest[:,1],yyxtest[:,0],linewidth=2, linestyle=\"-\", c=\"b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "#verileri(test, train target) 1 boyutlu vektör haline getiriyoruz.\n",
        "\n",
        "out_seq1 = a\n",
        "out_seq2 = rr\n",
        "# convert to [rows, columns] structure\n",
        "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
        "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
        "in_seq3 = in_seq3.reshape((len(in_seq3), 1))\n",
        "in_seq4 = in_seq4.reshape((len(in_seq4), 1))\n",
        "in_seq5 = in_seq5.reshape((len(in_seq5), 1))\n",
        "in_seq6 = in_seq6.reshape((len(in_seq6), 1))\n",
        "\n",
        "out_seq1 = out_seq1.reshape((len(out_seq1), 1))\n",
        "out_seq2 = out_seq2.reshape((len(out_seq2), 1))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "out_seqt1 = at\n",
        "out_seqt2 = rrt\n",
        "# convert to [rows, columns] structure\n",
        "in_seq1t = in_seq1t.reshape((len(in_seq1t), 1))\n",
        "in_seq2t = in_seq2t.reshape((len(in_seq2t), 1))\n",
        "in_seq3t = in_seq3t.reshape((len(in_seq3t), 1))\n",
        "in_seq4t = in_seq4t.reshape((len(in_seq4t), 1))\n",
        "in_seq5t = in_seq5t.reshape((len(in_seq5t), 1))\n",
        "in_seq6t = in_seq6t.reshape((len(in_seq6t), 1))\n",
        "\n",
        "out_seqt1 = out_seqt1.reshape((len(out_seqt1), 1))\n",
        "out_seqt2 = out_seqt2.reshape((len(out_seqt2), 1))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nfrom sklearn.preprocessing import MinMaxScaler\\nscaler=MinMaxScaler()\\nscaler.fit(train)\\nscaled_train = scaler.transform(train)\\nscaled_test = scaler.transform(test)\\ntrue_predictions = scaler.inverse_transform(test_predictions)\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'''\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler=MinMaxScaler()\n",
        "scaler.fit(train)\n",
        "scaled_train = scaler.transform(train)\n",
        "scaled_test = scaler.transform(test)\n",
        "true_predictions = scaler.inverse_transform(test_predictions)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JKBrGm0YPJaz"
      },
      "outputs": [],
      "source": [
        "# oluşturduumuz dikey datayı birleştiriyoruz\n",
        "dataset = hstack((in_seq1, in_seq2,in_seq3,in_seq4, in_seq5,in_seq6 ,out_seq1,out_seq2))\n",
        "\n",
        "datasettest = hstack((in_seq1t, in_seq2t,in_seq3t,in_seq4t, in_seq5t,in_seq6t, out_seqt1,out_seqt2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# choose a number of time steps(pencere boyutu)\n",
        "n_steps = 48"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jl1Hjlk3JVDq"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(192572, 48, 6)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test ve train dataları window size ı kullanarak 1d cnn e uygun hale getiriyoruz.\n",
        "X, y1,y2 = split_sequences(dataset, n_steps)\n",
        "Xt, yt1,yt2 = split_sequences(datasettest, n_steps)\n",
        "\n",
        "# one time series per head\n",
        "n_features = 1\n",
        "X.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''''\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "\n",
        "class MinMaxScaler3D(MinMaxScaler):\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        x = np.reshape(X, newshape=(X.shape[0]*X.shape[1], X.shape[2]))\n",
        "        return np.reshape(super().fit_transform(x, y=y), newshape=X.shape)\n",
        "''''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#scaler = MinMaxScaler3D()\n",
        "#X = scaler.fit_transform(X)\n",
        "#Xt = scaler.fit_transform(Xt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "A_iJ05P8JVAy"
      },
      "outputs": [],
      "source": [
        "# separate input data\n",
        "X1 = X[:, :, 0].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X2 = X[:, :, 1].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X3 = X[:, :, 2].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X4 = X[:, :, 3].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X5 = X[:, :, 4].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X6 = X[:, :, 5].reshape(X.shape[0], X.shape[1], n_features)\n",
        "X1t = Xt[:, :, 0].reshape(Xt.shape[0], Xt.shape[1], n_features)\n",
        "X2t = Xt[:, :, 1].reshape(Xt.shape[0], Xt.shape[1], n_features)\n",
        "X3t = Xt[:, :, 2].reshape(Xt.shape[0], Xt.shape[1], n_features)\n",
        "X4t = Xt[:, :, 3].reshape(Xt.shape[0], Xt.shape[1], n_features)\n",
        "X5t = Xt[:, :, 4].reshape(Xt.shape[0], Xt.shape[1], n_features)\n",
        "X6t = Xt[:, :, 5].reshape(Xt.shape[0], Xt.shape[1], n_features)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cMFbonQ6JU-K"
      },
      "outputs": [],
      "source": [
        "#1d cnn modeli bunu train ederken çalıştır\n",
        "'''\n",
        "inputax = Input(shape=(n_steps, n_features))\n",
        "inputay = Input(shape=(n_steps, n_features))\n",
        "inputaz = Input(shape=(n_steps, n_features))\n",
        "inputwx = Input(shape=(n_steps, n_features))\n",
        "inputwy = Input(shape=(n_steps, n_features))\n",
        "inputwz = Input(shape=(n_steps, n_features))\n",
        "act_func='LeakyReLU'\n",
        "filter1=1024\n",
        "filter2=512\n",
        "k_size1=24\n",
        "k_size2=16\n",
        "mp1=8\n",
        "mp2=4\n",
        "# first input model\n",
        "cnn1 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputax)\n",
        "cnn1 = MaxPooling1D(pool_size=mp1)(cnn1)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn1=Dropout(0.5)(cnn1)\n",
        "cnn1 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn1)\n",
        "cnn1 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn1)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn1=Dropout(0.5)(cnn1)\n",
        "#cnn1= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn1)\n",
        "cnn1 = Flatten()(cnn1)\n",
        "cnn1 = Model(inputs=inputax, outputs=cnn1)\n",
        "\n",
        "# second input model\n",
        "cnn2 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputay)\n",
        "cnn2 = MaxPooling1D(pool_size=mp1)(cnn2)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn2=Dropout(0.5)(cnn2)\n",
        "cnn2 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn2)\n",
        "cnn2 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn2)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn2=Dropout(0.5)(cnn2)\n",
        "#3cnn2= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn2)\n",
        "cnn2 = Flatten()(cnn2)\n",
        "cnn2 = Model(inputs=inputay, outputs=cnn2)\n",
        "\n",
        "\n",
        "\n",
        "cnn3 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputaz)\n",
        "cnn3 = MaxPooling1D(pool_size=mp1)(cnn3)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn3=Dropout(0.5)(cnn3)\n",
        "cnn3 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn3)\n",
        "cnn3 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn3)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn3=Dropout(0.5)(cnn3)\n",
        "#cnn3= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn3)\n",
        "cnn3 = Flatten()(cnn3)\n",
        "cnn3 = Model(inputs=inputaz, outputs=cnn3)\n",
        "\n",
        "#---------------yaw--------------------------------------------------------------------\n",
        "\n",
        "# first input model\n",
        "cnn4 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputwx)\n",
        "cnn4 = MaxPooling1D(pool_size=mp1)(cnn4)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn4=Dropout(0.5)(cnn4)\n",
        "cnn4 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn4)\n",
        "cnn4 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn4)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn4=Dropout(0.5)(cnn4)\n",
        "#cnn4= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn4)\n",
        "cnn4 = Flatten()(cnn4)\n",
        "cnn4 = Model(inputs=inputwx, outputs=cnn4)\n",
        "\n",
        "\n",
        "\n",
        "# second input model\n",
        "\n",
        "cnn5 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputwy)\n",
        "cnn5 = MaxPooling1D(pool_size=mp1)(cnn5)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn5=Dropout(0.5)(cnn5)\n",
        "cnn5 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn5)\n",
        "cnn5 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn5)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn5=Dropout(0.5)(cnn5)\n",
        "#cnn5= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn5)\n",
        "cnn5 = Flatten()(cnn5)\n",
        "cnn5 = Model(inputs=inputwy, outputs=cnn5)\n",
        "\n",
        "\n",
        "\n",
        "cnn6 = Conv1D(filters=filter1, kernel_size=k_size1, activation=act_func,padding='SAME')(inputwz)\n",
        "cnn6 = MaxPooling1D(pool_size=mp1)(cnn6)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn6=Dropout(0.5)(cnn6)\n",
        "cnn6 = Conv1D(filters=filter2, kernel_size=k_size2, activation=act_func,padding='SAME')(cnn6)\n",
        "cnn6 = MaxPooling1D(pool_size=mp2,padding='SAME')(cnn6)\n",
        "keras.layers.BatchNormalization(),\n",
        "cnn6=Dropout(0.5)(cnn6)\n",
        "#cnn6= GRU (32,dropout=0.5,recurrent_dropout=0.5)(cnn6)\n",
        "cnn6 = Flatten()(cnn6)\n",
        "cnn6= Model(inputs=inputwz, outputs=cnn6)\n",
        "\n",
        "\n",
        "# merge input models\n",
        "merge1 = concatenate([cnn1.output,cnn2.output,cnn3.output])\n",
        "merge2 = concatenate([cnn4.output,cnn5.output,cnn6.output])\n",
        "\n",
        "dense1 = Dense(256, activation=act_func)(merge1)\n",
        "keras.layers.BatchNormalization(),\n",
        "dense1=Dropout(0.5)(dense1)\n",
        "output1 = Dense(1)(dense1)\n",
        "\n",
        "dense2 = Dense(256, activation=act_func)(merge2)\n",
        "keras.layers.BatchNormalization(),\n",
        "dense2=Dropout(0.5)(dense2)\n",
        "output2 = Dense(1)(dense2)\n",
        "model= Model(inputs=[cnn1.input,cnn2.input,cnn3.input,cnn4.input,cnn5.input,cnn6.input],outputs=[output1,output2])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#eğitme işlemi yapar bunu train ederken çalıştır.\n",
        "'''\n",
        "optim = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.99, decay=0.01)\n",
        "\n",
        "model.compile(optimizer=optim,\n",
        "              loss=['mae','mae'],\n",
        "              metrics=['mae','mse'])\n",
        "\n",
        "es = EarlyStopping(monitor='loss',\n",
        "                   min_delta=1e-15,\n",
        "                   patience=10,\n",
        "                   verbose=1)\n",
        "\n",
        "rlr = ReduceLROnPlateau(monitor='val_loss',\n",
        "                        factor=0.001,\n",
        "                        patience=100,\n",
        "                        verbose=1)\n",
        "\n",
        "mcp = ModelCheckpoint(filepath=\"test.h5\",\n",
        "                      monitor='val_loss',\n",
        "                      verbose=1,\n",
        "                      save_best_only=True,\n",
        "                      save_weights_only=False)\n",
        "\n",
        "tb = TensorBoard('./logs')\n",
        "\n",
        "print('\\t Fitting Data into the Model...')\n",
        "history = model.fit([X1, X2,X3,X4,X5,X6], [y1,y2],\n",
        "                    epochs=1000,\n",
        "                    batch_size=2*n_steps,\n",
        "                    validation_data=([X1t,X2t,X3t,X4t,X5t,X6t], [yt1,yt2]),\n",
        "                    verbose=2,\n",
        "                    shuffle=False,\n",
        "                    callbacks=[mcp, rlr,es])\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "#önceden eğitilmiş modeli yükle\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('localization.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "YiPpyWl0JU0Z"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[7.0982846e-05],\n",
            "       [7.2367024e-05],\n",
            "       [7.1018585e-05],\n",
            "       ...,\n",
            "       [6.8454654e-05],\n",
            "       [6.9209142e-05],\n",
            "       [6.7868852e-05]], dtype=float32), array([[2.9069939e-05],\n",
            "       [3.4267247e-05],\n",
            "       [2.6977452e-05],\n",
            "       ...,\n",
            "       [3.2075484e-05],\n",
            "       [3.4349167e-05],\n",
            "       [3.6691759e-05]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "#norm deltaposition, deltaheading tahmini\n",
        "\n",
        "yhat = model.predict([X1t, X2t,X3t,X4t,X5t,X6t], verbose=0)\n",
        "print(yhat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tahmin ettiğimiz deltaheading ve deltaposition normu ayıralım\n",
        "yhat=np.asarray(yhat)\n",
        "yhat.shape\n",
        "yaw=yhat[1,:,0]\n",
        "mag=yhat[0,:,0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tahmin edilen deltaposition normun ilk 5000 datası\n",
        "arr = np.arange(0,len(mag[0:5000]))\n",
        "\n",
        "plt.scatter(arr,mag[0:5000],color='red')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#target deltaposition ilk 5000 datası\n",
        "arr = np.arange(0,len(at[0:5000]))\n",
        "plt.scatter(arr[:],at[0:5000],color='blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#tahmin  edilen delta yaw\n",
        "arr = np.arange(0,len(yaw))\n",
        "\n",
        "plt.scatter(arr,yaw,color='red')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#target deltayaw\n",
        "arr = np.arange(0,len(out_seqt2))\n",
        "plt.scatter(arr,out_seqt2,color='blue')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ccc=0\n",
        "arr = np.arange(0,len(yaw))\n",
        "xxrt=np.zeros(len(yaw))\n",
        "\n",
        "for i in arr:\n",
        "    xxrt[i]=ccc+yaw[i]\n",
        "    \n",
        "    ccc=xxrt[i]\n",
        "    \n",
        "\n",
        "plt.scatter(arr[:],xxrt,color='red') #tahmin edilen yaw(heading)\n",
        "arr1 = np.arange(0,len(yyxtest[:,2]))\n",
        "plt.scatter(arr1[:],yyxtest[:,2],color='blue')#target  yaw(heading)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#yaw ve normu kullanarak x,y yi hesaplayıp çizdiriyoruz\n",
        "arr = np.arange(0,len(xxrt))\n",
        "sx=np.zeros(len(xxrt))\n",
        "sy=np.zeros(len(xxrt))\n",
        "\n",
        "for i in arr:\n",
        "    sx[i]=mag[i]*math.cos(xxrt[i])\n",
        "    sy[i]=mag[i]*math.sin(xxrt[i])\n",
        "\n",
        "ddd=0\n",
        "vvv=0\n",
        "arr = np.arange(0,len(sx))\n",
        "xft=np.zeros((len(sx),2))\n",
        "\n",
        "\n",
        "for i in arr:\n",
        "    xft[i,0]=ddd+sx[i]\n",
        "    xft[i,1]=vvv+sy[i]\n",
        "    \n",
        "    ddd=xft[i,0]\n",
        "    vvv=xft[i,1]\n",
        "\n",
        "plt.scatter(xft[:,1],xft[:,0],linewidth=2, linestyle=\"-\", c=\"r\") #predicted (y,x grafiği) k\n",
        "\n",
        "plt.scatter(yyxtest[:,1],yyxtest[:,0],linewidth=2, linestyle=\"-\", c=\"b\") #target(y,x grafiği)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "plt.scatter(xft[:,0],xft[:,1],color='red') \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#from tensorflow.keras.models import load_model\n",
        "#model.save('localization.h5')  # creates a HDF5 file 'my_model.h5'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "keras-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8 (tags/v3.10.8:aaaf517, Oct 11 2022, 16:50:30) [MSC v.1933 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "f3f24c580f30fef3017907364422d96092de271b0bc2fc5f68547bbd206c01ff"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
